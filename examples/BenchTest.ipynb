{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from SafeRLBench import config\n",
    "from SafeRLBench import Bench, BenchConfig, BestPerformance\n",
    "from SafeRLBench.algo import PolicyGradient\n",
    "from SafeRLBench.envs import LinearCar, GeneralMountainCar\n",
    "from SafeRLBench import envs, tools, algo\n",
    "    \n",
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config.logger_set_level()\n",
    "config.logger_add_stream_handler()\n",
    "# config.logger_add_file_handler('BenchTestLog.log')\n",
    "config.monitor_set_verbosity(2)\n",
    "config.jobs_set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure a policy\n",
    "def policy_par(par):\n",
    "    def policy(x):\n",
    "        return np.exp(par[0] - par[1:].dot(x)) - 1.5\n",
    "    return policy\n",
    "policy = tools.Policy(policy_par, (3))\n",
    "\n",
    "def policy_grad(x, a, par):\n",
    "    return np.array([1., -x[0], -x[1]]) * policy_par(par)(x)\n",
    "\n",
    "policy_rif = tools.Policy(policy_par,(3), gradient=policy_grad, sigma=0.2)\n",
    "\n",
    "# configure environments\n",
    "algs = [\n",
    "    (PolicyGradient, [{'policy': policy, 'max_it': 1000, 'eps': 0.00001, 'estimator': 'central_fd'},\n",
    "                      {'policy': policy, 'max_it': 2000, 'eps': 0.00001, 'estimator': 'central_fd'}])\n",
    "]\n",
    "\n",
    "env = [[\n",
    "    (LinearCar, {'horizon': 200}),\n",
    "    (LinearCar, {'horizon': 100})\n",
    "]]\n",
    "\n",
    "\n",
    "test_config = BenchConfig(algs, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark = Bench(test_config, [BestPerformance()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark.measures[0].out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "benchmark.runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
