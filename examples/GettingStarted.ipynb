{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "This is an Notebook containing the examples from the **Getting Started** section in the documentation. Refer to the documentation for very verbose description of this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Optimizing a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the classes we need\n",
    "from SafeRLBench.envs import LinearCar\n",
    "from SafeRLBench.policy import LinearPolicy\n",
    "from SafeRLBench.algo import PolicyGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get an instance of `LinearCar` with the default arguments.\n",
    "linear_car = LinearCar()\n",
    "# we need a policy which maps R^2 to R\n",
    "policy = LinearPolicy(2, 1)\n",
    "# setup parameters\n",
    "policy.parameters = [-1, -1, 1]\n",
    "\n",
    "# plug the environment and policy into the algorithm\n",
    "optimizer = PolicyGradient(linear_car, policy, estimator='central_fd')\n",
    "\n",
    "# run optimization\n",
    "optimizer.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets take a look at what happened during the run. For this we can access the monitor and generate some plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = optimizer.monitor.rewards\n",
    "\n",
    "plt.plot(range(len(y)), y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the configuration object\n",
    "from SafeRLBench import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# setup stream handler\n",
    "config.logger_add_stream_handler()\n",
    "# setup logger level\n",
    "config.logger_set_level(config.DEBUG)\n",
    "# raise monitor verbosity\n",
    "config.monitor_set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After changing these values, please run the cell which invokes `optimizer.optimize` again to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the best performance measure\n",
    "from SafeRLBench.measure import BestPerformance\n",
    "# import the Bench and BenchConfig\n",
    "from SafeRLBench import Bench, BenchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define environment configuration.\n",
    "envs = [[(LinearCar, {'horizon': 100})]]\n",
    "# define algorithms configuration.\n",
    "algs = [[\n",
    "  (PolicyGradient, [{\n",
    "    'policy': LinearPolicy(2, 1, par=[-1, -1, 1]),\n",
    "    'estimator': 'central_fd',\n",
    "    'var': var\n",
    "  } for var in [1, 1.5, 2, 2.5]])\n",
    "]]\n",
    "\n",
    "# instantiate BenchConfig\n",
    "config = BenchConfig(algs, envs)\n",
    "\n",
    "# instantiate the bench\n",
    "bench = Bench(config, BestPerformance())\n",
    "\n",
    "# configure to run in parallel\n",
    "config.jobs_set(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bench()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bench.measures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_run = bench.measures[0].result[0][0]\n",
    "monitor = best_run.get_alg_monitor()\n",
    "best_trace = monitor.traces[monitor.rewards.index(max(monitor.rewards))]\n",
    "y = [t[1][0] for t in best_trace]\n",
    "x = range(len(y))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36-srb)",
   "language": "python",
   "name": "py36-srb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
